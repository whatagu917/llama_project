import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

# RAG ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
embedding_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# ã‚µãƒ³ãƒ—ãƒ«ã®ã‚³ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã«ã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã‚€ï¼‰
code_snippets = [
    "def quicksort(arr): return sorted(arr)",
    "def mergesort(arr): return merge(arr)",
    "def bubblesort(arr): return bubble(arr)"
]
file_paths = ["file1.py", "file2.py", "file3.py"]

# ã‚³ãƒ¼ãƒ‰ã‚’åŸ‹ã‚è¾¼ã¿ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰ã—ã¦ FAISS ã«ç™»éŒ²
embeddings = embedding_model.encode(code_snippets)
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(np.array(embeddings, dtype="float32"))

# âœ… `BitsAndBytesConfig` ã‚’ä½¿ç”¨ã—ã¦ 8bit é‡å­åŒ–ã‚’è¨­å®š
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,  # 8bit é‡å­åŒ–
    llm_int8_enable_fp32_cpu_offload=True  # CPU ã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ¡ãƒ¢ãƒªç¯€ç´„
)

# CodeLlama-13B ã‚’ 8bit é‡å­åŒ–ã§ãƒ­ãƒ¼ãƒ‰
model_name = "codellama/CodeLlama-13b-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",  # CPU & GPU ã‚’è‡ªå‹•æŒ¯ã‚Šåˆ†ã‘
    quantization_config=bnb_config,  # âœ… 8bit é‡å­åŒ–ã‚’é©ç”¨
    torch_dtype=torch.float16
)

# é¡ä¼¼ã‚³ãƒ¼ãƒ‰ã‚’æ¤œç´¢ã™ã‚‹é–¢æ•°
def search_similar_code(query, top_k=2):
    query_embedding = embedding_model.encode([query])
    distances, indices = index.search(query_embedding.astype("float32"), top_k)
    results = [(file_paths[i], code_snippets[i]) for i in indices[0]]
    return results

# Llama ã§ã‚³ãƒ¼ãƒ‰è£œå®Œã‚’ç”Ÿæˆ
def generate_code_with_context(query):
    similar_code = search_similar_code(query, top_k=2)
    context = "\n".join([f"# å‚è€ƒã‚³ãƒ¼ãƒ‰:\n{code}" for _, code in similar_code])
    prompt = f"{context}\n# è£œå®Œã—ãŸã„ã‚³ãƒ¼ãƒ‰:\n{query}"

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    output = model.generate(
        **inputs, 
        max_length=200,
        pad_token_id=tokenizer.eos_token_id  
    )
    
    
    return tokenizer.decode(output[0], skip_special_tokens=True)

# **å¯¾è©±å¼ã®å®Ÿè£…**
print("\nğŸ”¹ğŸ”¹ğŸ”¹ å¯¾è©±å‹ã‚³ãƒ¼ãƒ‰è£œå®Œã‚·ã‚¹ãƒ†ãƒ  ğŸ”¹ğŸ”¹ğŸ”¹")
print("ğŸ’¡ ã‚³ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€è£œå®Œçµæœã‚’è¡¨ç¤ºã—ã¾ã™ã€‚ï¼ˆçµ‚äº†ã™ã‚‹ã«ã¯ `exit` ã¨å…¥åŠ›ï¼‰\n")

while True:
    query = input("ğŸ“ ã‚³ãƒ¼ãƒ‰ã‚’å…¥åŠ›: ")
    if query.lower() == "exit":
        print("ğŸ”š çµ‚äº†ã—ã¾ã™ã€‚")
        break
    
    print("\nğŸ” é¡ä¼¼ã‚³ãƒ¼ãƒ‰ã‚’æ¤œç´¢ä¸­...")
    generated_code = generate_code_with_context(query)
    
    print("\nğŸ”¹ ğŸ”¹ ğŸ”¹ AI ãŒè£œå®Œã—ãŸã‚³ãƒ¼ãƒ‰ ğŸ”¹ ğŸ”¹ ğŸ”¹\n")
    print(generated_code)
    print("\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")
